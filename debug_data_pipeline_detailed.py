#!/usr/bin/env python3
"""
è¯¦ç»†è°ƒè¯•æ•°æ®è¯»å–æµç¨‹ï¼Œè¿½è¸ªæ¯ä¸ªæ­¥éª¤çš„æ•°æ®å˜åŒ–
"""

import pyrootutils
pyrootutils.setup_root(__file__, indicator='.project-root', pythonpath=True, dotenv=True)

import torch
import numpy as np
from common.data.data_utils import load_dataset
from hrdt.datasets.dataset import MultiDataCollatorForVLAConsumerDataset
from torch.utils.data import DataLoader
import omegaconf
from PIL import Image
import torchvision.transforms as T

def print_detailed_stats(tensor, name, step):
    """æ‰“å°è¯¦ç»†çš„tensorç»Ÿè®¡ä¿¡æ¯"""
    if isinstance(tensor, dict):
        print(f"\nğŸ” [{step}] {name} (dict):")
        for key, val in tensor.items():
            if isinstance(val, torch.Tensor):
                # è½¬æ¢ä¸ºfloatè¿›è¡Œç»Ÿè®¡è®¡ç®—
                val_float = val.float() if val.dtype in [torch.uint8, torch.int8, torch.int16, torch.int32, torch.int64] else val
                print(f"  {key}: shape={val.shape}, dtype={val.dtype}")
                print(f"       min={val_float.min():.6f}, max={val_float.max():.6f}, mean={val_float.mean():.6f}, std={val_float.std():.6f}")
    elif isinstance(tensor, torch.Tensor):
        # è½¬æ¢ä¸ºfloatè¿›è¡Œç»Ÿè®¡è®¡ç®—
        tensor_float = tensor.float() if tensor.dtype in [torch.uint8, torch.int8, torch.int16, torch.int32, torch.int64] else tensor
        print(f"\nğŸ” [{step}] {name}:")
        print(f"  shape={tensor.shape}, dtype={tensor.dtype}")
        print(f"  min={tensor_float.min():.6f}, max={tensor_float.max():.6f}, mean={tensor_float.mean():.6f}, std={tensor_float.std():.6f}")
        
        # æŒ‰é€šé“åˆ†æ
        if tensor.dim() >= 3 and tensor.shape[-1] == 3:  # å‡è®¾æœ€åä¸€ç»´æ˜¯RGB
            for c in range(3):
                channel_name = ['R', 'G', 'B'][c]
                channel_data = tensor_float[..., c]
                print(f"  {channel_name}: min={channel_data.min():.6f}, max={channel_data.max():.6f}, mean={channel_data.mean():.6f}")
    elif isinstance(tensor, np.ndarray):
        print(f"\nğŸ” [{step}] {name} (numpy):")
        print(f"  shape={tensor.shape}, dtype={tensor.dtype}")
        print(f"  min={tensor.min():.6f}, max={tensor.max():.6f}, mean={tensor.mean():.6f}, std={tensor.std():.6f}")

def debug_single_sample():
    """è°ƒè¯•å•ä¸ªæ ·æœ¬çš„å®Œæ•´æµç¨‹"""
    
    print("ğŸš€ å¼€å§‹è¯¦ç»†è°ƒè¯•æ•°æ®è¯»å–æµç¨‹...")
    
    # 1. ç›´æ¥ä»robotwinæ•°æ®é›†è·å–åŸå§‹æ•°æ®
    print("\n" + "="*60)
    print("æ­¥éª¤1: ç›´æ¥ä»robotwinæ•°æ®é›†è·å–åŸå§‹æ•°æ®")
    
    from hrdt.datasets.robotwin2.robotwin_agilex_dataset import RobotwinAgilexDataset
    
    # åˆ›å»ºrobotwinæ•°æ®é›†å®ä¾‹
    config = {
        'common': {
            'img_history_size': 1,
            'action_chunk_size': 16,
            'chunk_size': 16,
            'num_cameras': 3,
            'state_dim': 48,
            'action_dim': 48
        }
    }
    
    robotwin_dataset = RobotwinAgilexDataset(
        mode="multi_task",
        multi_task_root_dir="/dataset_rc_mm/share/datasets/huggingface.co/TianxingChen/RoboTwin2.0/dataset",
        config=config,
        val=False
    )
    
    # è·å–ä¸€ä¸ªåŸå§‹æ ·æœ¬
    raw_sample = robotwin_dataset.get_item()
    if raw_sample is None:
        print("âŒ æ— æ³•è·å–åŸå§‹æ ·æœ¬")
        return
    
    print("âœ… æˆåŠŸè·å–åŸå§‹æ ·æœ¬")
    
    # æ£€æŸ¥åŸå§‹å›¾åƒæ•°æ®
    if 'current_images' in raw_sample:
        current_imgs = raw_sample['current_images']  # numpy array
        print_detailed_stats(torch.from_numpy(current_imgs), "åŸå§‹current_images", "1a")
        
        # ä¿å­˜åŸå§‹å›¾åƒæ ·æœ¬
        if current_imgs.shape[0] > 0:  # æœ‰ç›¸æœºæ•°æ®
            sample_img = current_imgs[0, 0]  # ç¬¬ä¸€ä¸ªç›¸æœºï¼Œç¬¬ä¸€å¸§ (H, W, 3)
            if sample_img.max() > 1:  # å¦‚æœæ˜¯0-255èŒƒå›´
                sample_img_pil = Image.fromarray(sample_img.astype(np.uint8))
            else:  # å¦‚æœæ˜¯0-1èŒƒå›´
                sample_img_pil = Image.fromarray((sample_img * 255).astype(np.uint8))
            sample_img_pil.save("/dataset_rc_mm/chenby10@xiaopeng.com/Moto_copy/debug_output/raw_original.png")
            print(f"  ğŸ’¾ ä¿å­˜åŸå§‹å›¾åƒåˆ°: debug_output/raw_original.png")
    
    # 2. é€šè¿‡VLAConsumerDatasetå¤„ç†
    print("\n" + "="*60)
    print("æ­¥éª¤2: é€šè¿‡VLAConsumerDatasetå¤„ç†")
    
    dataset_config_path = "/dataset_rc_mm/chenby10@xiaopeng.com/Moto_copy/latent_motion_tokenizer/configs/data/hrdt_robotwin.yaml"
    extra_data_config = {
        'sequence_length': 1,
        'do_extract_future_frames': True,
        'do_extract_action': False
    }
    
    train_dataset, eval_dataset = load_dataset(dataset_config_path, extra_data_config)
    
    # è·å–ä¸€ä¸ªå¤„ç†åçš„æ ·æœ¬
    processed_sample = eval_dataset[0]
    
    if 'images' in processed_sample:
        images = processed_sample['images']
        print_detailed_stats(images, "VLAConsumerDatasetå¤„ç†åçš„images", "2a")
        
        # å¦‚æœæ˜¯dictæ ¼å¼ï¼Œæ£€æŸ¥pixel_values
        if isinstance(images, dict) and 'pixel_values' in images:
            pixel_values = images['pixel_values']
            print_detailed_stats(pixel_values, "pixel_values", "2b")
            
            # ä¿å­˜å¤„ç†åçš„å›¾åƒ
            if pixel_values.dim() >= 3:
                # å–ç¬¬ä¸€ä¸ªæ ·æœ¬
                sample_tensor = pixel_values[0] if pixel_values.dim() == 4 else pixel_values
                
                # å°è¯•ånormalizationçœ‹çœ‹
                imagenet_mean = torch.tensor([0.485, 0.456, 0.406]).view(3, 1, 1)
                imagenet_std = torch.tensor([0.229, 0.224, 0.225]).view(3, 1, 1)
                
                # ånormalization
                denorm_tensor = sample_tensor * imagenet_std + imagenet_mean
                denorm_tensor = torch.clamp(denorm_tensor, 0, 1)
                
                print_detailed_stats(denorm_tensor, "ånormalizationå", "2c")
                
                # ä¿å­˜å›¾åƒ
                denorm_pil = T.ToPILImage()(denorm_tensor)
                denorm_pil.save("/dataset_rc_mm/chenby10@xiaopeng.com/Moto_copy/debug_output/processed_denorm.png")
                print(f"  ğŸ’¾ ä¿å­˜ånormalizationå›¾åƒåˆ°: debug_output/processed_denorm.png")
    
    # 3. é€šè¿‡DataLoaderå¤„ç†
    print("\n" + "="*60)
    print("æ­¥éª¤3: é€šè¿‡DataLoaderå¤„ç†")
    
    collator = MultiDataCollatorForVLAConsumerDataset(
        unified_action_dim=48, 
        use_precomp_lang_embed=True
    )
    
    eval_dataloader = DataLoader(
        eval_dataset,
        batch_size=1,
        shuffle=False,
        collate_fn=collator,
        num_workers=0,
        pin_memory=False
    )
    
    batch = next(iter(eval_dataloader))
    
    if 'rgb_initial' in batch:
        print_detailed_stats(batch['rgb_initial'], "DataLoaderè¾“å‡ºçš„rgb_initial", "3a")
    if 'rgb_future' in batch:
        print_detailed_stats(batch['rgb_future'], "DataLoaderè¾“å‡ºçš„rgb_future", "3b")
    
    # 4. æ£€æŸ¥image_transformå‡½æ•°
    print("\n" + "="*60)
    print("æ­¥éª¤4: æ£€æŸ¥image_transformå‡½æ•°")
    
    from common.data.data_utils import create_hrdt_image_transform
    
    # åˆ›å»ºä¸€ä¸ªæµ‹è¯•å›¾åƒ
    test_img = Image.new('RGB', (224, 224), (128, 128, 128))  # ä¸­ç­‰ç°åº¦
    
    transform_fn = create_hrdt_image_transform((224, 224))
    transformed = transform_fn(test_img)
    
    print_detailed_stats(transformed, "transformå‡½æ•°è¾“å‡º", "4a")
    
    # 5. æ‰‹åŠ¨æµ‹è¯•normalization
    print("\n" + "="*60)
    print("æ­¥éª¤5: æ‰‹åŠ¨æµ‹è¯•normalization")
    
    # åˆ›å»ºä¸€ä¸ªå·²çŸ¥çš„æµ‹è¯•tensor
    test_tensor = torch.ones(3, 224, 224) * 0.5  # ä¸­ç­‰äº®åº¦
    print_detailed_stats(test_tensor, "æµ‹è¯•tensor (0.5äº®åº¦)", "5a")
    
    # åº”ç”¨ImageNet normalization
    imagenet_mean = torch.tensor([0.485, 0.456, 0.406]).view(3, 1, 1)
    imagenet_std = torch.tensor([0.229, 0.224, 0.225]).view(3, 1, 1)
    
    normalized = (test_tensor - imagenet_mean) / imagenet_std
    print_detailed_stats(normalized, "æ‰‹åŠ¨normalizationå", "5b")
    
    print("\n" + "="*60)
    print("ğŸ¯ è°ƒè¯•å®Œæˆï¼è¯·æ£€æŸ¥ä¸Šè¿°è¾“å‡ºæ‰¾å‡ºé—®é¢˜æ‰€åœ¨ã€‚")

if __name__ == "__main__":
    debug_single_sample()
