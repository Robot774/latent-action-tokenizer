#!/usr/bin/env python3
"""
Hand Keypoint Visualizer for EgoDex Dataset
ä¸¥æ ¼æŒ‰ç…§ visualize_2d.py çš„å®ç°æ–¹å¼ï¼Œåªé€‚é…ä¸åŒçš„è¾“å…¥æ ¼å¼
"""

import numpy as np
import cv2
import json
from pathlib import Path
from typing import Tuple, List, Optional, Dict, Any
import torch


class HandKeypointVisualizer:
    """æ‰‹éƒ¨å…³é”®ç‚¹å¯è§†åŒ–å™¨ - åŸºäº visualize_2d.py"""
    
    def __init__(self, fps: int = 30):
        """
        åˆå§‹åŒ–å¯è§†åŒ–å™¨
        
        Args:
            fps: è§†é¢‘å¸§ç‡
        """
        self.fps = fps
        
        # ä¸¥æ ¼æŒ‰ç…§visualize_2d.pyçš„é¢œè‰²æ˜ å°„
        self.finger_colors = {
            'little': (0, 152, 191),    # light blue
            'ring': (173, 255, 47),     # green yellow  
            'middle': (230, 245, 250),  # pale torquoise
            'index': (255, 99, 71),     # tomato
            'thumb': (238, 130, 238)    # violet
        }
        
    def denormalize_actions(self, 
                          normalized_actions: np.ndarray, 
                          stat_path: str) -> np.ndarray:
        """åå½’ä¸€åŒ–åŠ¨ä½œæ•°æ®"""
        with open(stat_path, 'r') as f:
            stats = json.load(f)
        
        action_min = np.array(stats['egodex']['min'])
        action_max = np.array(stats['egodex']['max'])
        
        # åå½’ä¸€åŒ–: x_real = (x_norm + 1) / 2 * (max - min) + min
        denormalized = (normalized_actions + 1) / 2 * (action_max - action_min) + action_min
        
        return denormalized
        
    def parse_48d_to_transforms(self, action_48d: np.ndarray) -> Dict[str, np.ndarray]:
        """
        å°†48DåŠ¨ä½œæ•°æ®è½¬æ¢ä¸ºå˜æ¢çŸ©é˜µæ ¼å¼ (æ¨¡æ‹Ÿvisualize_2d.pyçš„tfsæ ¼å¼)
        
        Args:
            action_48d: (48,) 48ç»´åŠ¨ä½œå‘é‡
            
        Returns:
            transforms: åŒ…å«æ‰‹éƒ¨å’ŒæŒ‡å°–å˜æ¢çŸ©é˜µçš„å­—å…¸
        """
        transforms = {}
        
        # è§£æ48Dæ•°æ®
        # å·¦æ‰‹æ•°æ® (å‰24ç»´)
        left_data = action_48d[:24]
        left_wrist_pos = left_data[:3]           # ä½ç½® (3D)
        left_wrist_rot = left_data[3:9]          # æ—‹è½¬ 6D (æš‚ä¸ä½¿ç”¨)
        left_fingertips = left_data[9:24].reshape(5, 3)  # 5ä¸ªæŒ‡å°– (5x3D)
        
        # å³æ‰‹æ•°æ® (å24ç»´)  
        right_data = action_48d[24:48]
        right_wrist_pos = right_data[:3]         # ä½ç½® (3D)
        right_wrist_rot = right_data[3:9]        # æ—‹è½¬ 6D (æš‚ä¸ä½¿ç”¨)
        right_fingertips = right_data[9:24].reshape(5, 3)  # 5ä¸ªæŒ‡å°– (5x3D)
        
        # åˆ›å»º4x4å˜æ¢çŸ©é˜µ (æŒ‰ç…§visualize_2d.pyçš„æ ¼å¼)
        def create_transform_matrix(position):
            """åˆ›å»º4x4å˜æ¢çŸ©é˜µï¼Œä½ç½®ä¿¡æ¯åœ¨æœ€åä¸€åˆ—"""
            tf = np.eye(4)
            tf[:3, 3] = position  # ä½ç½®å­˜å‚¨åœ¨æœ€åä¸€åˆ— (å’Œvisualize_2d.pyä¸€è‡´)
            return tf
        
        # å·¦æ‰‹å˜æ¢çŸ©é˜µ
        transforms['leftHand'] = create_transform_matrix(left_wrist_pos)
        transforms['leftThumbTip'] = create_transform_matrix(left_fingertips[0])
        transforms['leftIndexFingerTip'] = create_transform_matrix(left_fingertips[1])
        transforms['leftMiddleFingerTip'] = create_transform_matrix(left_fingertips[2])
        transforms['leftRingFingerTip'] = create_transform_matrix(left_fingertips[3])
        transforms['leftLittleFingerTip'] = create_transform_matrix(left_fingertips[4])
        
        # å³æ‰‹å˜æ¢çŸ©é˜µ
        transforms['rightHand'] = create_transform_matrix(right_wrist_pos)
        transforms['rightThumbTip'] = create_transform_matrix(right_fingertips[0])
        transforms['rightIndexFingerTip'] = create_transform_matrix(right_fingertips[1])
        transforms['rightMiddleFingerTip'] = create_transform_matrix(right_fingertips[2])
        transforms['rightRingFingerTip'] = create_transform_matrix(right_fingertips[3])
        transforms['rightLittleFingerTip'] = create_transform_matrix(right_fingertips[4])
        
        return transforms
        
    def convert_to_camera_frame(self, transforms_dict: Dict[str, np.ndarray], 
                               cam_ext: np.ndarray) -> Dict[str, np.ndarray]:
        """
        ä¸¥æ ¼æŒ‰ç…§visualize_2d.pyçš„convert_to_camera_frameå®ç°
        
        Args:
            transforms_dict: å˜æ¢çŸ©é˜µå­—å…¸
            cam_ext: ç›¸æœºå¤–å‚ (4, 4)
            
        Returns:
            transforms_in_cam: ç›¸æœºåæ ‡ç³»ä¸‹çš„å˜æ¢çŸ©é˜µå­—å…¸
        """
        # ä¸¥æ ¼æŒ‰ç…§ utils/data_utils.py: return np.linalg.inv(cam_ext)[None] @ tfs
        world_to_cam = np.linalg.inv(cam_ext)
        
        transforms_in_cam = {}
        for name, tf in transforms_dict.items():
            # åº”ç”¨ç›¸æœºå˜æ¢
            transforms_in_cam[name] = world_to_cam @ tf
            
        return transforms_in_cam
        
    def draw_line(self, pointa: np.ndarray, pointb: np.ndarray, 
                  image: np.ndarray, intrinsic: np.ndarray, 
                  color: Tuple[int, int, int] = (0, 255, 0), thickness: int = 5):
        """
        ä¸¥æ ¼æŒ‰ç…§visualize_2d.pyçš„draw_lineå®ç°
        """
        # ä¸¥æ ¼æŒ‰ç…§åŸç‰ˆï¼šproject 3d points into 2d
        pointa2, _ = cv2.projectPoints(pointa, np.eye(3), np.zeros(3), intrinsic, distCoeffs=np.zeros(5))  
        pointb2, _ = cv2.projectPoints(pointb, np.eye(3), np.zeros(3), intrinsic, distCoeffs=np.zeros(5))  
        pointa2 = pointa2.squeeze()
        pointb2 = pointb2.squeeze()

        # ä¸¥æ ¼æŒ‰ç…§åŸç‰ˆï¼šdon't draw if the line is out of bounds
        H, W, _ = image.shape
        if (pointb2[0] < 0 and pointa2[0] > W) or (pointb2[1] < 0 and pointa2[1] > H) or (pointa2[0] < 0 and pointb2[0] > W) or (pointa2[1] < 0 and pointb2[1] > H):
            return 

        # ä¸¥æ ¼æŒ‰ç…§åŸç‰ˆï¼šdraws a line in-place
        cv2.line(image, pointa2.astype(int), pointb2.astype(int), color=color, thickness=thickness)
        cv2.circle(image, (int(pointa2[0]), int(pointa2[1])), 15, color, -1)
        cv2.circle(image, (int(pointb2[0]), int(pointb2[1])), 15, color, -1)

    def draw_line_sequence(self, points_list: List[np.ndarray], 
                          image: np.ndarray, intrinsic: np.ndarray, 
                          color: Tuple[int, int, int] = (0, 255, 0)):
        """
        ä¸¥æ ¼æŒ‰ç…§visualize_2d.pyçš„draw_line_sequenceå®ç°
        """
        # ä¸¥æ ¼æŒ‰ç…§åŸç‰ˆï¼šdraw a sequence of lines in-place
        ptm = points_list[0]
        for pt in points_list[1:]:
            self.draw_line(ptm, pt, image, intrinsic, color)
            ptm = pt

    def get_finger_pts(self, finger_names: List[str], 
                      transforms_in_cam: Dict[str, np.ndarray], 
                      hand_name: str) -> List[np.ndarray]:
        """
        ä¸¥æ ¼æŒ‰ç…§visualize_2d.pyçš„get_finger_ptså®ç°
        
        Args:
            finger_names: æŒ‡å°–åç§°åˆ—è¡¨ (å¦‚ ['rightThumbTip'])
            transforms_in_cam: ç›¸æœºåæ ‡ç³»ä¸‹çš„å˜æ¢çŸ©é˜µ
            hand_name: æ‰‹è…•åç§° (å¦‚ 'rightHand')
        """
        # ä¸¥æ ¼æŒ‰ç…§åŸç‰ˆï¼šgrab 3D position from SE(3) pose
        finger_points = [transforms_in_cam[hand_name][:3, 3]]  # æ‰‹è…•ä½ç½®
        for tfname in finger_names:
            finger_points.append(transforms_in_cam[tfname][:3, 3])  # æŒ‡å°–ä½ç½®

        return finger_points
        
    def draw_hand(self, hand_dict: Dict[str, List[str]], 
                  transforms_in_cam: Dict[str, np.ndarray], 
                  cam_img: np.ndarray, cam_int: np.ndarray, 
                  right: bool = True):
        """
        ä¸¥æ ¼æŒ‰ç…§visualize_2d.pyçš„draw_handå®ç°
        """
        hand_name = 'rightHand' if right else 'leftHand'
        
        # ä¸¥æ ¼æŒ‰ç…§åŸç‰ˆï¼šdraw fingers
        for finger in ['little', 'ring', 'middle', 'index', 'thumb']:  # roughly stack lines so closer fingers are drawn on top
            if finger in hand_dict:
                points = self.get_finger_pts(hand_dict[finger], transforms_in_cam, hand_name)
                self.draw_line_sequence(points, cam_img, cam_int,
                                      color=self.finger_colors[finger])
        
    def create_frame_with_keypoints(self, 
                                  action_48d: np.ndarray,
                                  camera_extrinsics: np.ndarray,
                                  camera_intrinsics: np.ndarray,
                                  image_height: int = 1080,
                                  image_width: int = 1920,
                                  background_color: Tuple[int, int, int] = (0, 0, 0)) -> np.ndarray:
        """
        åˆ›å»ºåŒ…å«æ‰‹éƒ¨å…³é”®ç‚¹çš„å•å¸§å›¾åƒ (æŒ‰ç…§visualize_2d.pyçš„ä¸»æµç¨‹)
        """
        # åˆ›å»ºèƒŒæ™¯å›¾åƒ
        cam_img = np.full((image_height, image_width, 3), background_color, dtype=np.uint8)
        
        # ç¬¬ä¸€æ­¥ï¼šå°†48Dè½¬æ¢ä¸ºå˜æ¢çŸ©é˜µæ ¼å¼
        transforms = self.parse_48d_to_transforms(action_48d)
        
        # ç¬¬äºŒæ­¥ï¼šè½¬æ¢åˆ°ç›¸æœºåæ ‡ç³» (ä¸¥æ ¼æŒ‰ç…§visualize_2d.py)
        transforms_in_cam = self.convert_to_camera_frame(transforms, camera_extrinsics)
        
        # ç¬¬ä¸‰æ­¥ï¼šå®šä¹‰æ‰‹æŒ‡æ˜ å°„ (ä¸¥æ ¼æŒ‰ç…§visualize_2d.py)
        right_dict = {
            'index': ['rightIndexFingerTip'], 
            'thumb': ['rightThumbTip'], 
            'middle': ['rightMiddleFingerTip'], 
            'ring': ['rightRingFingerTip'], 
            'little': ['rightLittleFingerTip']
        }
        left_dict = {
            'index': ['leftIndexFingerTip'], 
            'thumb': ['leftThumbTip'], 
            'middle': ['leftMiddleFingerTip'], 
            'ring': ['leftRingFingerTip'], 
            'little': ['leftLittleFingerTip']
        }

        # ç¬¬å››æ­¥ï¼šç»˜åˆ¶æ‰‹éƒ¨ (ä¸¥æ ¼æŒ‰ç…§visualize_2d.py)
        self.draw_hand(right_dict, transforms_in_cam, cam_img, camera_intrinsics, right=True)
        self.draw_hand(left_dict, transforms_in_cam, cam_img, camera_intrinsics, right=False)

        return cam_img

    def imgs_to_mp4(self, img_list: List[np.ndarray], mp4_path: str, 
                   fps: int = 30, fourcc=None):
        """
        ä½¿ç”¨ PyAV æ¥å†™å…¥è§†é¢‘ï¼Œé¿å… OpenCV VideoWriter çš„ç¼–è§£ç å™¨é—®é¢˜
        """
        if not img_list:
            return
            
        H, W, _ = img_list[0].shape
        
        try:
            import av
            
            # åˆ›å»ºè¾“å‡ºå®¹å™¨
            container = av.open(mp4_path, mode='w')
            
            # æ·»åŠ è§†é¢‘æµ
            stream = container.add_stream('libx264', rate=fps)
            stream.width = W
            stream.height = H
            stream.pix_fmt = 'yuv420p'
            
            for i, img in enumerate(img_list):
                # ç¡®ä¿å›¾åƒæ˜¯ RGB æ ¼å¼ä¸”ä¸º uint8
                if img.dtype != np.uint8:
                    img = img.astype(np.uint8)
                
                # åˆ›å»º PyAV å¸§
                frame = av.VideoFrame.from_ndarray(img, format='rgb24')
                frame.pts = i
                
                # ç¼–ç å¸§
                for packet in stream.encode(frame):
                    container.mux(packet)
            
            # åˆ·æ–°ç¼–ç å™¨
            for packet in stream.encode():
                container.mux(packet)
                
            # å…³é—­å®¹å™¨
            container.close()
                
        except ImportError:
            # å›é€€åˆ°åŸæ¥çš„ OpenCV æ–¹æ³•
            self._imgs_to_mp4_opencv(img_list, mp4_path, fps)
        except Exception as e:
            # å›é€€åˆ° OpenCV
            self._imgs_to_mp4_opencv(img_list, mp4_path, fps)
    
    def _imgs_to_mp4_opencv(self, img_list: List[np.ndarray], mp4_path: str, fps: int = 30):
        """OpenCV çš„åå¤‡æ–¹æ³•ï¼Œå°è¯•å¤šç§ç¼–è§£ç å™¨"""
        H, W, _ = img_list[0].shape
        
        # å°è¯•ä¸åŒçš„ç¼–è§£ç å™¨
        codecs = [
            cv2.VideoWriter_fourcc(*'mp4v'),
            cv2.VideoWriter_fourcc(*'XVID'),
            cv2.VideoWriter_fourcc(*'MJPG'),
            -1
        ]
        
        for fourcc in codecs:
            video_out = cv2.VideoWriter(mp4_path, fourcc, fps, (W, H))
            
            if video_out.isOpened():
                for img in img_list:
                    BGR_img = cv2.cvtColor(img, cv2.COLOR_RGB2BGR)
                    video_out.write(BGR_img)
                video_out.release()
                
                # æ£€æŸ¥æ–‡ä»¶æ˜¯å¦æˆåŠŸåˆ›å»º
                import os
                if os.path.exists(mp4_path) and os.path.getsize(mp4_path) > 0:
                    return

    def visualize_action_sequence(self, 
                                actions_48d: np.ndarray,
                                camera_extrinsics: np.ndarray,
                                camera_intrinsics: np.ndarray,
                                output_path: str,
                                image_height: int = 1080,
                                image_width: int = 1920,
                                is_normalized: bool = True,
                                stat_path: Optional[str] = None) -> None:
        """
        å¯è§†åŒ–48DåŠ¨ä½œåºåˆ—å¹¶ç”Ÿæˆè§†é¢‘ (æŒ‰ç…§visualize_2d.pyçš„ä¸»æµç¨‹)
        """
        # åå½’ä¸€åŒ– (å¦‚æœéœ€è¦)
        if is_normalized:
            if stat_path is None:
                raise ValueError("Normalized data requires stat_path for denormalization")
            actions_48d = self.denormalize_actions(actions_48d, stat_path)
        
        print(f"ğŸ¬ å¼€å§‹ç”Ÿæˆè§†é¢‘: {output_path}")
        print(f"   - å¸§æ•°: {len(actions_48d)}")
        print(f"   - åˆ†è¾¨ç‡: {image_width}x{image_height}")
        print(f"   - å¸§ç‡: {self.fps}fps")
        
        # ä¸¥æ ¼æŒ‰ç…§visualize_2d.pyçš„ä¸»å¾ªç¯
        out_imgs = []
        for i, (action_48d, cam_ext) in enumerate(zip(actions_48d, camera_extrinsics)):
            # åˆ›å»ºåŒ…å«å…³é”®ç‚¹çš„å¸§
            cam_img = self.create_frame_with_keypoints(
                action_48d, cam_ext, camera_intrinsics,
                image_height=image_height, image_width=image_width
            )
            
            out_imgs.append(cam_img)
            
            if (i + 1) % 10 == 0:
                print(f"   - å·²å¤„ç†: {i+1}/{len(actions_48d)} å¸§")

        # ä¸¥æ ¼æŒ‰ç…§visualize_2d.pyçš„è§†é¢‘å†™å…¥
        self.imgs_to_mp4(out_imgs, output_path, fps=self.fps)
        print(f"âœ… è§†é¢‘ç”Ÿæˆå®Œæˆ: {output_path}")


def visualize_egodx_sample(sample_data: Dict[str, Any], 
                          output_path: str,
                          stat_path: str,
                          image_size: Tuple[int, int] = (1920, 1080),
                          fps: int = 30) -> None:
    """
    å¯è§†åŒ–EgoDxæ•°æ®é›†æ ·æœ¬çš„æ¥å£å‡½æ•°
    """
    # åˆ›å»ºå¯è§†åŒ–å™¨
    visualizer = HandKeypointVisualizer(fps=fps)
    
    # æå–æ•°æ®
    actions = sample_data['actions']  # (chunk_size, 48)
    camera_extrinsics = sample_data['action_camera_extrinsics']  # (chunk_size, 4, 4)
    camera_intrinsics = sample_data['camera_intrinsics']  # (3, 3)
    
    # è½¬æ¢ä¸ºnumpy (å¦‚æœæ˜¯tensor)
    if torch.is_tensor(actions):
        actions = actions.cpu().numpy()
    if torch.is_tensor(camera_extrinsics):
        camera_extrinsics = camera_extrinsics.cpu().numpy()
    if torch.is_tensor(camera_intrinsics):
        camera_intrinsics = camera_intrinsics.cpu().numpy()
    
    # ç”Ÿæˆå¯è§†åŒ–è§†é¢‘
    visualizer.visualize_action_sequence(
        actions_48d=actions,
        camera_extrinsics=camera_extrinsics,
        camera_intrinsics=camera_intrinsics,
        output_path=output_path,
        image_height=image_size[1],
        image_width=image_size[0],
        is_normalized=True,  # EgoDxæ•°æ®é›†ä½¿ç”¨å½’ä¸€åŒ–
        stat_path=stat_path
    )


if __name__ == "__main__":
    print("ğŸ§ª Hand Keypoint Visualizer - åŸºäº visualize_2d.py å®ç°")
