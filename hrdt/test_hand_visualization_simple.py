#!/usr/bin/env python3
"""
ç®€åŒ–çš„æ‰‹éƒ¨å…³é”®ç‚¹å¯è§†åŒ–æµ‹è¯• - ç§»é™¤é”™è¯¯æ•è·ä¾¿äºè°ƒè¯•
"""

import pyrootutils
pyrootutils.setup_root(__file__, indicator='.project-root', pythonpath=True, dotenv=True)

import os
import sys
import torch
import numpy as np
from torchvision import transforms
from PIL import Image

# æ·»åŠ å¯è§†åŒ–æ¨¡å—å¯¼å…¥
sys.path.append('Moto_copy/hrdt')
from visualization.hand_keypoint_visualizer import HandKeypointVisualizer, visualize_egodx_sample
from hrdt.datasets.dataset import VLAConsumerDataset

def create_simple_image_transform():
    """Create simple image transform for testing"""
    def transform_fn(image):
        if image is None:
            image = Image.new('RGB', (224, 224), (0, 0, 0))
        
        transform = transforms.Compose([
            transforms.Resize((224, 224)),
            transforms.ToTensor(),
            transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])
        ])
        
        pixel_values = transform(image)
        return {"pixel_values": pixel_values}
    
    return transform_fn

def test_hand_visualization():
    """ç®€åŒ–çš„æ‰‹éƒ¨å…³é”®ç‚¹å¯è§†åŒ–æµ‹è¯• - æ— é”™è¯¯æ•è·"""
    print("ğŸ¬ Testing Hand Keypoint Visualization (Direct Debug Mode)...")
    
    # é…ç½®
    base_config = {
        "common": {
            "img_history_size": 1,
            "action_chunk_size": 16,
            "chunk_size": 16,
            "num_cameras": 1,
            "state_dim": 48,
            "action_dim": 48
        }
    }
    
    image_transform = create_simple_image_transform()
    
    # åˆ›å»ºæ•°æ®é›†
    print("ğŸ”§ Creating EgoDx dataset...")
    egodx_dataset = VLAConsumerDataset(
        config=base_config,
        image_transform=image_transform,
        num_cameras=1,
        dataset_name="egodex",
        dataset_type="pretrain",
        image_aug=False,
        upsample_rate=3,
        val=True,
        use_precomp_lang_embed=True,
    )
    
    print(f"âœ… Dataset created (size: {len(egodx_dataset)})")
    
    # è·å–æ ·æœ¬
    print("ğŸ“¦ Getting sample...")
    sample = egodx_dataset.get_item(0)
    
    print("âœ… Sample obtained")
    print("ğŸ“Š Sample structure:")
    for key, value in sample.items():
        if isinstance(value, torch.Tensor):
            print(f"  {key}: {value.shape} ({value.dtype})")
        elif isinstance(value, np.ndarray):
            print(f"  {key}: {value.shape} ({value.dtype})")
        else:
            print(f"  {key}: {type(value)}")
    
    # æ£€æŸ¥å¿…è¦å­—æ®µ
    required_fields = ['actions', 'camera_intrinsics']
    camera_ext_fields = ['action_camera_extrinsics', 'current_camera_extrinsics']
    
    has_required = all(field in sample for field in required_fields)
    has_camera_ext = any(field in sample for field in camera_ext_fields)
    
    if not has_required or not has_camera_ext:
        print("âŒ Missing required fields")
        return
    
    print("âœ… All required fields present")
    
    # åŸºç¡€å¯è§†åŒ–å™¨æµ‹è¯•
    print("ğŸ¨ Testing basic visualizer...")
    visualizer = HandKeypointVisualizer(fps=10)
    
    # æµ‹è¯•æ•°æ®å‡†å¤‡
    if isinstance(sample['actions'], torch.Tensor):
        test_action = sample['actions'][0].cpu().numpy()
    else:
        test_action = sample['actions'][0]
    
    # æµ‹è¯•48Dè§£æ
    print("ğŸ”„ Testing 48D parsing...")
    parsed_transforms = visualizer.parse_48d_to_transforms(test_action)
    print(f"âœ… Parsed {len(parsed_transforms)} transforms")
    
    # å‡†å¤‡ç›¸æœºå‚æ•°
    if 'action_camera_extrinsics' in sample:
        camera_extrinsics = sample['action_camera_extrinsics']
    else:
        current_ext = sample['current_camera_extrinsics']
        action_length = sample['actions'].shape[0]
        if isinstance(current_ext, torch.Tensor):
            camera_extrinsics = current_ext.repeat(action_length, 1, 1)
        else:
            camera_extrinsics = np.repeat(current_ext, action_length, axis=0)
    
    camera_intrinsics = sample['camera_intrinsics']
    
    # è½¬æ¢ä¸ºnumpy
    if torch.is_tensor(camera_extrinsics):
        camera_extrinsics = camera_extrinsics.cpu().numpy()
    if torch.is_tensor(camera_intrinsics):
        camera_intrinsics = camera_intrinsics.cpu().numpy()
    
    # æµ‹è¯•ç›¸æœºå˜æ¢
    print("ğŸ”„ Testing camera transform...")
    transforms_in_cam = visualizer.convert_to_camera_frame(parsed_transforms, camera_extrinsics[0])
    print("âœ… Camera transform successful")
    
    # æµ‹è¯•å•å¸§ç”Ÿæˆ
    print("ğŸ–¼ï¸ Testing frame generation...")
    frame = visualizer.create_frame_with_keypoints(
        test_action, 
        camera_extrinsics[0], 
        camera_intrinsics,
        image_height=1080,
        image_width=1920
    )
    print(f"âœ… Frame generated: {frame.shape}")
    
    # æµ‹è¯•è§†é¢‘ç”Ÿæˆ
    print("ğŸ¬ Testing video generation...")
    output_path = "/workspace/chenby10@xiaopeng.com/test_hand_keypoints_debug.mp4"
    stat_path = "/workspace/chenby10@xiaopeng.com/Moto_copy/hrdt/datasets/pretrain/egodex_stat.json"
    
    # åªç”¨å‰3å¸§æµ‹è¯•
    short_actions = sample['actions'][:15]
    short_camera_ext = camera_extrinsics[:15]
    
    short_sample = {
        'actions': short_actions,
        'action_camera_extrinsics': short_camera_ext,
        'camera_intrinsics': camera_intrinsics
    }
    
    # ç›´æ¥è°ƒç”¨ï¼Œæ— é”™è¯¯æ•è·
    visualize_egodx_sample(
        sample_data=short_sample,
        output_path=output_path,
        stat_path=stat_path,
        image_size=(1920, 1080),  # ä½¿ç”¨åŸå§‹å›¾åƒå°ºå¯¸
        fps=5
    )
    
    # æ£€æŸ¥ç»“æœ
    if os.path.exists(output_path):
        file_size = os.path.getsize(output_path) / (1024 * 1024)
        print(f"âœ… Video created: {output_path}")
        print(f"   File size: {file_size:.2f} MB")
        print(f"   Frames: {len(short_actions)}")
    else:
        print("âŒ Video not created")
    
    print("ğŸ‰ Test completed!")

if __name__ == "__main__":
    test_hand_visualization()
